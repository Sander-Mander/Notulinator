{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sander-Mander/Notulinator/blob/main/Notulinator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M-tJh2SVCPU2",
      "metadata": {
        "id": "M-tJh2SVCPU2"
      },
      "source": [
        "#Notulinator 🤖"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kmJonShmiUx6",
      "metadata": {
        "id": "kmJonShmiUx6"
      },
      "source": [
        "**Welcome to the Notulinator!**\n",
        "\n",
        "Before we start, go to Runtime > Change runtime type and select T4 GPU. Save your selection.\n",
        "\n",
        "Wait until you are connected. You can see that you are connected if a green checkmark appears in the top right of your screen.\n",
        "\n",
        "\n",
        "You use the Notulinator by running one code block at a time. You can run a code block by hovering over the code block and pressing the arrow button in the top left of the block. <u>Do not change anything in the code itself!</u> After running a code block, <u>instructions will appear below the block</u>. Listen to the instructions from the little robot 🤖. Follow\n",
        "these instructions and you will have created your transcription in no-time!\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ibuLtsJvHvDZ",
      "metadata": {
        "id": "ibuLtsJvHvDZ"
      },
      "source": [
        "#Code (<-- Run this before you follow the steps! This takes about 5 minutes.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch torchvision torchaudio -y -q\n",
        "!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121 -q\n",
        "\n",
        "!pip install ctranslate2==4.4.0 -q\n",
        "!pip install faster-whisper==1.1.0 -q\n",
        "!pip install whisperx==3.3.1 -q\n",
        "\n",
        "!apt-get update -q\n",
        "!apt-get install libcudnn8=8.9.2.26-1+cuda12.1 -q\n",
        "!apt-get install libcudnn8-dev=8.9.2.26-1+cuda12.1 -q\n",
        "\n",
        "!python -c \"import torch; torch.backends.cuda.matmul.allow_tf32 = True; torch.backends.cudnn.allow_tf32 = True\""
      ],
      "metadata": {
        "id": "EUEWgWQeQ75m"
      },
      "id": "EUEWgWQeQ75m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "FB7eswS37veP",
      "metadata": {
        "id": "FB7eswS37veP"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import gc\n",
        "import datetime\n",
        "import csv\n",
        "\n",
        "class BaseBot:\n",
        "    def show_message(self, text, delay=0):\n",
        "      \"\"\"Display a robot message with optional delay\"\"\"\n",
        "      print(f'🤖: {text}')\n",
        "      if delay > 0:\n",
        "          time.sleep(delay)\n",
        "    def clear(self):\n",
        "      clear_output(wait=True)\n",
        "\n",
        "class UploadBot(BaseBot):\n",
        "    def __init__(self):\n",
        "        self.path_name = 'Notulinator-Audio'\n",
        "        self.file_path = f'/content/gdrive/MyDrive/{self.path_name}'\n",
        "        self.selected_file = None\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the interactive helper flow\"\"\"\n",
        "        self.clear()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          self.show_message('Hello! Time to get started with your minutes.', 2)\n",
        "          self.preprocessing_choice()\n",
        "        else:\n",
        "          self.show_message(\"You are not connected to a GPU. Go to Runtime > Change runtime type and select T4 GPU. Then run the above code and this code block again.\")\n",
        "\n",
        "    def preprocessing_choice(self):\n",
        "        \"\"\"Ask user if they want to preprocess the audio\"\"\" #TODO: Skip the ask to make more user friendly\n",
        "        self.show_message(\"Do you wish to pre-process your audio file first (highly recommended) or do you want to upload it as-is? The AI needs audio that is not too quiet, else it will fail.\")\n",
        "\n",
        "        preproc_yes = widgets.Button(description='Pre-process first')\n",
        "        preproc_no = widgets.Button(description='No pre-processing')\n",
        "\n",
        "        preproc_yes.on_click(lambda b: self.handle_preprocessing(True))\n",
        "        preproc_no.on_click(lambda b: self.handle_preprocessing(False))\n",
        "\n",
        "        display(preproc_yes, preproc_no)\n",
        "\n",
        "    def handle_preprocessing(self, preprocess):\n",
        "        \"\"\"Handle the preprocessing choice\"\"\"\n",
        "        self.clear()\n",
        "\n",
        "        if preprocess:\n",
        "            self.show_message(\"Good choice to first process your audio. However, I cannot do this myself; you will have to do this manually.\", 2)\n",
        "            self.show_message(\"Please look at the instructions at the very bottom of this document. Here is it described how you can quickly process your audio using Audacity!\", 1)\n",
        "        else:\n",
        "            self.mount_drive()\n",
        "\n",
        "    def mount_drive(self):\n",
        "        \"\"\"Mount Google Drive and set up folders\"\"\"\n",
        "        self.show_message(\"A pop-up should now appear to get access to your Google Drive. Please accept and wait a few seconds...\", 1)\n",
        "\n",
        "        drive.mount('/content/gdrive')\n",
        "\n",
        "        os.makedirs(self.file_path, exist_ok=True)\n",
        "\n",
        "        self.show_message(f\"I created a folder in your Google Drive called {self.path_name}. Please go to your Google Drive and upload your audio file in this folder. It should be saved in: {self.file_path}\")\n",
        "\n",
        "        done_button = widgets.Button(description='Done')\n",
        "        done_button.on_click(lambda b: self.check_uploaded_files())\n",
        "        display(done_button)\n",
        "\n",
        "    def check_uploaded_files(self):\n",
        "        \"\"\"Check for uploaded files in the designated folder\"\"\"\n",
        "        self.clear()\n",
        "        file_names = os.listdir(self.file_path) #TODO: Ignore unsupported file formats\n",
        "\n",
        "        if len(file_names) == 0:\n",
        "            self.show_message(f\"I found no files in the {self.path_name} folder. Can you check if you uploaded anything?\")\n",
        "            done_button = widgets.Button(description='Check Again')\n",
        "            done_button.on_click(lambda b: self.check_uploaded_files())\n",
        "            display(done_button)\n",
        "\n",
        "        elif len(file_names) == 1:\n",
        "            self.selected_file = file_names[0]\n",
        "            self.file_selected()\n",
        "\n",
        "        else:\n",
        "            self.show_file_selection(file_names)\n",
        "\n",
        "    def show_file_selection(self, file_names):\n",
        "        \"\"\"Display radio buttons for file selection\"\"\"\n",
        "        self.show_message(f\"I found multiple files, which one do you want to process:\")\n",
        "\n",
        "        radio_buttons = widgets.RadioButtons(\n",
        "            options=file_names,\n",
        "            description='Choose file:',\n",
        "        )\n",
        "        confirm_button = widgets.Button(description='Confirm')\n",
        "\n",
        "        confirm_button.on_click(lambda b: self.handle_file_selection(radio_buttons.value))\n",
        "\n",
        "        display(radio_buttons, confirm_button)\n",
        "\n",
        "    def handle_file_selection(self, filename):\n",
        "        \"\"\"Handle the file selection\"\"\"\n",
        "        self.clear()\n",
        "        self.selected_file = filename\n",
        "        self.file_selected()\n",
        "\n",
        "    def file_selected(self):\n",
        "        \"\"\"Process the selected file\"\"\"\n",
        "        audio_path = os.path.join(self.file_path, self.selected_file)\n",
        "        self.show_message(f\"Ready to process the following file: {self.selected_file}\", 1)\n",
        "        self.show_message(\"You can now move on to the next code cell! (Step 2)\")\n",
        "\n",
        "        global FILE_PATH, TOKEN_PATH\n",
        "        FILE_PATH = os.path.join(self.file_path, self.selected_file)\n",
        "        TOKEN_PATH = os.path.join(self.file_path, 'access_token.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "erpvR1AT6n24",
      "metadata": {
        "id": "erpvR1AT6n24"
      },
      "outputs": [],
      "source": [
        "class AccessBot(BaseBot):\n",
        "  def __init__(self):\n",
        "    self.token_input = None\n",
        "\n",
        "  def start(self):\n",
        "    self.clear()\n",
        "    self.show_message('To use some of the AI models, you have to request access. Have you already requested access to the models?', 1)\n",
        "    self.show_message('If not, I will guide you through the process.')\n",
        "    self.choice_access()\n",
        "\n",
        "  def choice_access(self):\n",
        "    access_yes = widgets.Button(description='Yes I have')\n",
        "    access_no = widgets.Button(description='No I have not')\n",
        "\n",
        "    access_yes.on_click(lambda b: self.handle_access(True))\n",
        "    access_no.on_click(lambda b: self.handle_access(False))\n",
        "\n",
        "    display(access_yes, access_no)\n",
        "\n",
        "  def handle_access(self, access):\n",
        "    self.clear()\n",
        "    if access:\n",
        "      self.show_message('Great, if you already have access you can move on to the next code cell! (Step 3)')\n",
        "    else:\n",
        "      self.show_message('Alright, since we are using AI models from HuggingFace (a large AI community platform), you have to create an account there.', 2)\n",
        "      self.show_message('Go to huggingface.co and create an account. Press Done when you are ready!')\n",
        "      done_button = widgets.Button(description='Done')\n",
        "      done_button.on_click(lambda b: self.request_access())\n",
        "      display(done_button)\n",
        "\n",
        "  def request_access(self):\n",
        "    self.clear()\n",
        "    self.show_message('Great! Now you have to request access to the specific AI models we will be using.', 2)\n",
        "    self.show_message('Request access to: https://huggingface.co/pyannote/speaker-diarization-3.1', 1)\n",
        "    self.show_message('And request access to: https://huggingface.co/pyannote/segmentation-3.0',1)\n",
        "    self.show_message('You can request access by filling in some basic contact information.')\n",
        "    done_button = widgets.Button(description='Done')\n",
        "    done_button.on_click(lambda b: self.create_token())\n",
        "    display(done_button)\n",
        "\n",
        "  def create_token(self):\n",
        "    self.clear()\n",
        "    self.show_message('Super, we are almost done. We still have to generate an access token so we can actually use these models in our code.', 3)\n",
        "    self.show_message('Go to your HuggingFace account and go to Settings > Access Tokens > Create new token.', 2)\n",
        "    self.show_message('You should set your token settings like this:', 1)\n",
        "    self.show_message('''Token type = Fine-grained\n",
        "    Token name can be anything you want (but do give it a name)\n",
        "    Under User permissions, repositories:\n",
        "    Enable 'Read access to contents of all repos under your personal namespace' AND 'Read access to contents of all public gated repos you can access'.''', 2)\n",
        "    self.show_message(\"Finally, click 'Create Token' to create your token\")\n",
        "    done_button = widgets.Button(description='Done')\n",
        "    done_button.on_click(lambda b: self.ask_token())\n",
        "    display(done_button)\n",
        "\n",
        "  def ask_token(self):\n",
        "    self.clear()\n",
        "    self.show_message('Paste the generated token here:')\n",
        "    self.token_input = widgets.Text(\n",
        "    description=\"Token:\",\n",
        "    placeholder=\"Enter your Hugging Face token\",)\n",
        "    save_button = widgets.Button(description=\"Save Token\")\n",
        "    save_button.on_click(lambda b: self.save_token())\n",
        "    display(self.token_input, save_button)\n",
        "\n",
        "  def save_token(self):\n",
        "    self.clear()\n",
        "    with open(TOKEN_PATH, \"w\") as f:\n",
        "      f.write(self.token_input.value)\n",
        "      self.show_message('Beep. Token saved in your Google Drive. You can move on to the next code cell! (Step 3)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "26trMHFjS1os",
      "metadata": {
        "id": "26trMHFjS1os"
      },
      "outputs": [],
      "source": [
        "class TranscriptBot(BaseBot):\n",
        "  def __init__(self, batch_size=16, language='en', print=True, model='large-v3', task='transcribe'):\n",
        "    self.batch_size = batch_size\n",
        "    self.language = language\n",
        "    self.print = print\n",
        "    self.model = model\n",
        "    self.task = task\n",
        "    self.device = 'cuda'\n",
        "    self.compute_type = 'float16'\n",
        "    self.access_token = None\n",
        "    self.result = None\n",
        "\n",
        "  def start(self):\n",
        "    self.clear() #TODO: Error when no file path specified\n",
        "    self.show_message('Now it is finally time to start the transcription!')\n",
        "    with open(TOKEN_PATH, \"r\") as f:\n",
        "      self.access_token = f.read().strip() #TODO: Give error when no token found\n",
        "\n",
        "    print(self.access_token)\n",
        "    print(FILE_PATH)\n",
        "    self.transcribe()\n",
        "\n",
        "  def transcribe(self):\n",
        "    self.show_message('Importing the AI models...')\n",
        "    torch.cuda.empty_cache()\n",
        "    model_whisper = whisperx.load_model(whisper_arch=self.model, device=self.device, compute_type=self.compute_type, language=self.language, task=self.task)\n",
        "    audio = whisperx.load_audio(FILE_PATH)\n",
        "\n",
        "    self.show_message('Starting the transcription...')\n",
        "    self.result = model_whisper.transcribe(audio, batch_size=self.batch_size)\n",
        "\n",
        "    self.show_message('Aligning the transcription...')\n",
        "    model_a, metadata = whisperx.load_align_model(language_code=self.result[\"language\"], device=self.device)\n",
        "    self.result = whisperx.align(self.result[\"segments\"], model_a, metadata, audio, self.device, return_char_alignments=False)\n",
        "\n",
        "    gc.collect(); torch.cuda.empty_cache(); del model_a #Remove model to save memory\n",
        "\n",
        "    self.show_message('Assigning speakers to the transcription...')\n",
        "    diarize_model = whisperx.DiarizationPipeline(use_auth_token=self.access_token, device=self.device)\n",
        "    diarize_segments = diarize_model(audio)\n",
        "    self.result = whisperx.assign_word_speakers(diarize_segments, self.result)\n",
        "    self.print_and_save()\n",
        "\n",
        "  def print_and_save(self):\n",
        "    result_intermediate = []\n",
        "    for segment in self.result['segments']:\n",
        "        start = datetime.timedelta(seconds=round(segment['start'], 0))\n",
        "        end = datetime.timedelta(seconds=round(segment['end'], 0))\n",
        "        speaker = segment.get('speaker', 'UNKNOWN')\n",
        "        text = segment['text'].strip()\n",
        "        if self.print:\n",
        "          print(f\"{speaker}: {text}\")\n",
        "\n",
        "        result_intermediate.append((f\"{start} to {end}\", f\"{speaker}: {text}\"))\n",
        "\n",
        "    txt_file = \"transcription-notulinator.txt\"\n",
        "    with open(txt_file, mode='w', encoding='utf-8') as file:\n",
        "        # Write data rows\n",
        "        for time_interval, transcription in result_intermediate:\n",
        "            file.write(f\"[{time_interval}]\\t{transcription}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "byA-XsSMHzsp",
      "metadata": {
        "id": "byA-XsSMHzsp"
      },
      "source": [
        "#Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mMO-1lHaH4O4",
      "metadata": {
        "id": "mMO-1lHaH4O4"
      },
      "source": [
        "**Step 1: Uploading your audio file** 📝"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yFGYvM-gH2xY",
      "metadata": {
        "id": "yFGYvM-gH2xY"
      },
      "outputs": [],
      "source": [
        "UploadBot().start()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tijfLT9dH9xR",
      "metadata": {
        "id": "tijfLT9dH9xR"
      },
      "source": [
        "**Step 2: Getting access to the AI models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nyeUGVaFIB8V",
      "metadata": {
        "id": "nyeUGVaFIB8V"
      },
      "outputs": [],
      "source": [
        "AccessBot().start()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4qkir6YAIHUL",
      "metadata": {
        "id": "4qkir6YAIHUL"
      },
      "source": [
        "**Step 3: Creating your transcription**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisperx\n",
        "TranscriptBot(language='en').start()"
      ],
      "metadata": {
        "id": "QG7QWo0bQMv3"
      },
      "id": "QG7QWo0bQMv3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "OO_JBBEM4_2p",
      "metadata": {
        "id": "OO_JBBEM4_2p"
      },
      "source": [
        "---\n",
        "\n",
        "**Pre-processing**\n",
        "\n",
        "To get a good speech-to-text transcription, the audio needs to be as intelligible as possible. The model performs bad on quiet audio. For many audio files, it is therefore important to first do some processing on the audio recording.\n",
        "\n",
        "1. Download Audacity (or any other audio processing software/site with dynamic range compression)\n",
        "2. File > Import > Audio, and select your audio file(s). If you have multiple files, it is easiest to move them to the same track.\n",
        "3. Find a section of the recording where the audio is loud and easy to hear, and check the decibel level of that audio. Then find a section of the recording where the audio speech is very silent, and check the decibel level of that audio. Do Ctrl + A, go to Effect > Volume and Compression > Compressor. Put your threshold to a decibel level **below** the well intelligible audio and **above** the poorly intelligible audio. Set the ratio quite high (something like 6:1, but this depends on your audio). Click Apply.\n",
        "5. (Optional) If the silent audio sections are still quite quiet, increase the overall volume of the audio by going to Effect > Volume and Compression > Amplify. Amplify it such that silent sections are easy to hear, but loud talkers are not so loud that it becomes distorted (when the volume line becomes red).\n",
        "6. (Optional) For convenience, it is recommended to export to audio into multiple smaller files (60 mins or so). Place your selection line at the part where you want to split the audio. Go to Edit > Labels > Add Label at Selection. You can do this at multiple locations to split the file into multiple seperate audio files.\n",
        "7. Go to File > Export Audio. Give your file a name and specify a folder. Choose Channels: **Mono** and Sample Rate: **16000 Hz**. If you want to export the audio to multiple files (Step 6), for Export Range select Multiple Files. Split files based on labels, include audio before first label, and choose a naming format that you like. Click Export.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Common Problems:**\n",
        "\n",
        "\n",
        "\n",
        "*   *My transcription results are bad.*\n",
        "\n",
        "The main cause of poor results is bad audio quality. Especially if voices are silent, the AI model struggles with transcription. Be sure to pre-process your audio beforehand so that silent audio sections become louder (as described above). The model also struggles with multiple people speaking at once, for which there is not really a fix.\n",
        "\n",
        "*   *I want to use a different languange than English.*\n",
        "\n",
        "The model supports many different languages, and can also translate between them. You can check under 'DEFAULT_ALIGN_MODELS_TORCH' and 'DEFAULT_ALIGN_MODELS_HF' what languages are available in this section of the code: https://github.com/m-bain/whisperX/blob/main/whisperx/alignment.py. For example, setting language='nl' in Step 3 sets the model to Dutch.\n",
        "\n",
        "*   *I am getting an error when running the code.*\n",
        "\n",
        "Be sure you have ran all the code above the code block you want to run. Errors can often be fixed by deleting and restarting your runtime. Do this by going to Runtime > Disconnect and Delete Runtime, then run the code again. If this does not resolve the error then you can open a New Issue on the Notulinator GitHub and I can take a look."
      ],
      "metadata": {
        "id": "NwBxDDeUYWcW"
      },
      "id": "NwBxDDeUYWcW"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}